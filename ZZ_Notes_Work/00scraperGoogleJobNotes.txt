# from urllib import request
# from bs4 import BeautifulSoup
# import csv

# class scraperGoogleJob():
#     print("Made it this far!!")
    
#     #def read_job_data(job_data):
#     def convert_csv_data(job_data):
#         with open ('job_data.csv', mode='r') as file:
#             reader = csv.reader(file)
#             csv_data = []
#             for row in job_data:
#                 csv_data.append(row)
#                 #print(csv_data)
#         return 0
    
#     def write_to_csv(job_data):
#         with open ('job_data.csv', mode='a', newline='') as file:
#             writer = csv.writer(file)
#             #for row in writer:
#             writer.writerow(job_data)
#         return 0
    
    
    
    
    
#     def get_job_info(job_link):
#         print("Made it to the get_job_info method!")
#         for jobs in job_link:
#             result = request.get(jobs)
#             content = result.text
#             soup = BeautifulSoup(content, 'lxml')
            
#             print(soup.prettify())
            
#         if job_link == "jobs.lever.co":
#             #lever_io(job_link, soup)
#             scraperGoogleJob.lever_io(job_link, soup)
#             #  Captcha
#             #<input id="hcaptchaResponseInput" type="hidden" name="h-captcha-response" value>
#             #<button id="hcaptchaSubmitBtn" type="submit" class="hidden"></button>
            
#         elif job_link == "boards.greenhouse.io":
#             scraperGoogleJob.greenhouse_io(soup)

#         elif job_link == "workday":
#             scraperGoogleJob.workday(soup)
#         return "ok"
    
    
    
    
    
    
    
    
    
            
#     #filter out already applied jobs
#     #traverse job webpage
#     #?????
#     def lever_io_data(job_link, soup):
#         opening_link_application = soup.find('div', class_='page-application')      #application immediate
#         opening_link_description = soup.find('div', class_='page-show')             #regular description start
        
#         #if soup.find('a', class_='main-header-logo'):
#         if opening_link_application:
#             try:
#                 company_open_positions = soup.find('a', class_='main-header-logo')
#                 plethora_of_jobs = company_open_positions['href']
#                 print(plethora_of_jobs)
#             except:
#                 #TODO: Change this Error type!
#                 raise ConnectionError("ERROR: Companies other open positions are not present")
#         elif opening_link_application:
#             try:
#                 position_title = soup.find('h2').get_text()
#                 job_title = position_title.split()
#                 print(job_title)
#                 job_info = job_title.nextSibling('div', class_="posting-categories")
#                 job_location = job_info.find('div', class_='location')
#                 job_department = job_info.find('div', class_='department')
#                 job_commitment = job_info.find('div', class_='commitment')
#                 job_style = job_info.find('div', class_='workplaceTypes')
        
        
#                 job_apply_butt = soup.find('a', {'data-qa': 'btn-apply-bottom'})
#                 link_to_apply = job_apply_butt['href']
#                 print(link_to_apply)

#             except:
#                 #TODO: Change this Error type!
#                 raise ConnectionError("ERROR: Companies other open positions are not present")
                
#         return soup

#     def lever_io_apply(application_link, application_webpage_html):
#         job_form_html = application_webpage_html.find("form", id="application-form", method="POST")
#         application_section_html = job_form_html.find_all("h4")
#         #using ^this list .find() 1st <h4> then increment...
#         print(application_section_html)
#         for user_input in application_section_html:
#             #loop through <input> tags and fill in using selenium!!
#             print(user_input)
#         return 0
    
#     def greenhouse_io_data(soup):
#         print("Here")
#         return 0
    
#     def workday_data(soup):
#         print("Here")
#         return 0
    
#     def apply_to_job(job_data: list):
#         if len(job_data)-1:
#             return "ok"
            














#4444
# from urllib import request
# from bs4 import BeautifulSoup
# import csv

# class scraperGoogleJob():
#     print("Made it this far!!")
    
#     #def read_job_data(job_data):
#     def convert_csv_data(job_data):
#         with open ('job_data.csv', mode='r') as file:
#             reader = csv.reader(file)
#             csv_data = []
#             for row in job_data:
#                 csv_data.append(row)
#                 #print(csv_data)
#         return 0
    
#     def write_to_csv(job_data):
#         with open ('job_data.csv', mode='a', newline='') as file:
#             writer = csv.writer(file)
#             #for row in writer:
#             writer.writerow(job_data)
#         return 0
    
#     @classmethod
#     def get_job_info(cls, job_link):   #param == job_link
#          print("Made it to the get_job_info method!")
#          job_link = ["https://jobs.lever.co/rover/0a6bcb57-bc8b-4826-b98c-7a17cbb4a911/apply", "https://jobs.lever.co/palantir/e82b696e-a085-4bbf-8bcb-6d2c4f8cf2f7"]
#          for job in job_link:
#              result = request.get(job)
#              content = result.text
#              soup = BeautifulSoup(content, 'lxml')
#              print(soup.prettify())
            
#              if "jobs.lever.co" in job:
#                  scraperGoogleJob.lever_io(job, soup)
#                  # Captcha
#                  # <input id="hcaptchaResponseInput" type="hidden" name="h-captcha-response" value>
#                  # <button id="hcaptchaSubmitBtn" type="submit" class="hidden"></button>
            
#              elif "boards.greenhouse.io" in job:
#                  scraperGoogleJob.greenhouse_io(soup)

#              elif "workday" in job:
#                  scraperGoogleJob.workday(soup)
                
#          return "ok"
    
    
#     #filter out already applied jobs
#     #traverse job webpage
#     #?????
#     def lever_io_data(job_link, soup):
#         opening_link_application = soup.find('div', class_='page-application')      #application immediate
#         opening_link_description = soup.find('div', class_='page-show')             #regular description start
        
#         #if soup.find('a', class_='main-header-logo'):
#         if opening_link_application:
#             try:
#                 company_open_positions = soup.find('a', class_='main-header-logo')
#                 plethora_of_jobs = company_open_positions['href']
#                 print(plethora_of_jobs)
#             except:
#                 #TODO: Change this Error type!
#                 raise ConnectionError("ERROR: Companies other open positions are not present")
#         elif opening_link_application:
#             try:
#                 position_title = soup.find('h2').get_text()
#                 job_title = position_title.split()
#                 print(job_title)
#                 job_info = job_title.nextSibling('div', class_="posting-categories")
#                 job_location = job_info.find('div', class_='location')
#                 job_department = job_info.find('div', class_='department')
#                 job_commitment = job_info.find('div', class_='commitment')
#                 job_style = job_info.find('div', class_='workplaceTypes')
        
        
#                 job_apply_butt = soup.find('a', {'data-qa': 'btn-apply-bottom'})
#                 link_to_apply = job_apply_butt['href']
#                 print(link_to_apply)

#             except:
#                 #TODO: Change this Error type!
#                 raise ConnectionError("ERROR: Companies other open positions are not present")
                
#         return soup

#     def lever_io_apply(application_link, application_webpage_html):
#         job_form_html = application_webpage_html.find("form", id="application-form", method="POST")
#         application_section_html = job_form_html.find_all("h4")
#         #using ^this list .find() 1st <h4> then increment...
#         print(application_section_html)
#         for user_input in application_section_html:
#             #loop through <input> tags and fill in using selenium!!
#             print(user_input)
#         return 0
    
#     def greenhouse_io_data(soup):
#         print("Here")
#         return 0
    
#     def workday_data(soup):
#         print("Here")
#         return 0
    
#     def apply_to_job(job_data: list):
#         if len(job_data)-1:
#             return "ok"



















# 5555555
# from urllib import request
# from bs4 import BeautifulSoup
# import csv

# class scraperGoogleJob:
#     print("Made it this far!!")
    
#     @staticmethod
#     def convert_csv_data(job_data):
#         with open('job_data.csv', mode='r') as file:
#             reader = csv.reader(file)
#             csv_data = []
#             for row in job_data:
#                 csv_data.append(row)
#                 #print(csv_data)
#         return 0
    
#     @staticmethod
#     def write_to_csv(job_data):
#         with open('job_data.csv', mode='a', newline='') as file:
#             writer = csv.writer(file)
#             #for row in writer:
#             writer.writerow(job_data)
#         return 0
    
#     @staticmethod
#     def get_job_info(job_link):
#         print("Made it to the get_job_info method!")
#         for job in job_link:
#             result = request.get(job)
#             content = result.text
#             soup = BeautifulSoup(content, 'lxml')
#             print(soup.prettify())
            
#             if "jobs.lever.co" in job:
#                 scraperGoogleJob.lever_io(job, soup)
#                 # Captcha
#                 # <input id="hcaptchaResponseInput" type="hidden" name="h-captcha-response" value>
#                 # <button id="hcaptchaSubmitBtn" type="submit" class="hidden"></button>
            
#             elif "boards.greenhouse.io" in job:
#                 scraperGoogleJob.greenhouse_io(soup)

#             elif "workday" in job:
#                 scraperGoogleJob.workday(soup)
                
#         return "ok"
    
    
#     @staticmethod
#     def lever_io(job_link, soup):
#         opening_link_application = soup.find('div', class_='page-application')      #application immediate
#         opening_link_description = soup.find('div', class_='page-show')             #regular description start
        
#         #if soup.find('a', class_='main-header-logo'):
#         if opening_link_application:
#             try:
#                 company_open_positions = soup.find('a', class_='main-header-logo')
#                 plethora_of_jobs = company_open_positions['href']
#                 print(plethora_of_jobs)
#             except:
#                 #TODO: Change this Error type!
#                 raise ConnectionError("ERROR: Companies other open positions are not present")
#         elif opening_link_application:
#             try:
#                 position_title = soup.find('h2').get_text()
#                 job_title = position_title.split()
#                 print(job_title)
#                 job_info = job_title.nextSibling('div', class_="posting-categories")
#                 job_location = job_info.find('div', class_='location')
#                 job_department = job_info.find('div', class_='department')
#                 job_commitment = job_info.find('div', class_='commitment')
#                 job_style = job_info.find('div', class_='workplaceTypes')
        
        
#                 job_apply_butt = soup.find('a', {'data-qa': 'btn-apply-bottom'})
#                 link_to_apply = job_apply_butt['href']
#                 print(link_to_apply)

#             except:
#                 #TODO: Change this Error type!
#                 raise ConnectionError("ERROR: Companies other open positions are not present")
                
#         return soup

#     @staticmethod
#     def lever_io_apply(application_link,



























#666666
from urllib import request
from bs4 import BeautifulSoup
import csv

class scraperGoogleJob:
    print("Made it this far!!")
    
    #def read_job_data(job_data):
    def convert_csv_data(job_data):
        with open ('job_data.csv', mode='r') as file:
            reader = csv.reader(file)
            csv_data = []
            for row in job_data:
                csv_data.append(row)
                #print(csv_data)
        return 0
    
    def write_to_csv(job_data):
        with open ('job_data.csv', mode='a', newline='') as file:
            writer = csv.writer(file)
            #for row in writer:
            writer.writerow(job_data)
        return 0
    
    @classmethod
    def get_job_info(cls, job_link):
        print("Made it to the get_job_info method!")
        job_links = ["https://jobs.lever.co/rover/0a6bcb57-bc8b-4826-b98c-7a17cbb4a911/apply", "https://jobs.lever.co/palantir/e82b696e-a085-4bbf-8bcb-6d2c4f8cf2f7"]
        for job in job_links:
            result = request.get(job)
            content = result.text
            soup = BeautifulSoup(content, 'lxml')
            print(soup.prettify())
            
            if "jobs.lever.co" in job:
                cls.lever_io(job, soup)
                # Captcha
                # <input id="hcaptchaResponseInput" type="hidden" name="h-captcha-response" value>
                # <button id="hcaptchaSubmitBtn" type="submit" class="hidden"></button>
            
            elif "boards.greenhouse.io" in job:
                cls.greenhouse_io(soup)

            elif "workday" in job:
                cls.workday(soup)
                
        return "ok"
    
    
    #filter out already applied jobs
    #traverse job webpage
    #?????
    def lever_io_data(job_link, soup):
        opening_link_application = soup.find('div', class_='page-application')      #application immediate
        opening_link_description = soup.find('div', class_='page-show')             #regular description start
        
        #if soup.find('a', class_='main-header-logo'):
        if opening_link_application:
            try:
                company_open_positions = soup.find('a', class_='main-header-logo')
                plethora_of_jobs = company_open_positions['href']
                print(plethora_of_jobs)
            except:
                #TODO: Change this Error type!
                raise ConnectionError("ERROR: Companies other open positions are not present")
        elif opening_link_application:
            try:
                position_title = soup.find('h2').get_text()
                job_title = position_title.split()
                print(job_title)
                job_info = job_title.nextSibling('div', class_="posting-categories")
                job_location = job_info.find('div', class_='location')
                job_department = job_info.find('div', class_='department')
                job_commitment = job_info.find('div', class_='commitment')
                job_style = job_info.find('div', class_='workplaceTypes')
        
        
                job_apply_butt = soup.find('a', {'data-qa': 'btn-apply-bottom'})
                link_to_apply = job_apply_butt['href']
                print(link_to_apply)

            except:
                #TODO: Change this Error type!
                raise ConnectionError("ERROR: Companies other open positions are not present")
                
        return soup

    def lever_io_apply(application_link, application_webpage_html):
        job_form_html = application_webpage_html.find("form", id="application-form", method="POST")
        application_section_html = job_form_html.find_all("h4")
        #using ^this list .find() 1st <h4> then increment...
        print(application_section_html)
        for user_input in application_section_html:
            #loop through <input> tags and fill in using selenium!!
            print(user_input)
        return 0
    
    def greenhouse_io_data(soup):
        print("Here")
        return 0
    
    def workday_data(soup):
        print("Here")
        return 0
    
    def apply_to_job(job_data: list):
        if len(job_data)-1:
            return "ok"



                
#if __name__ == '__main__':
#    scraper = scraperGoogleJob()












------------------------------------------------------------------------------------------------------------------------------------












from urllib import request
#from bs4 import BeautifulSoup
import csv

class scraperGoogleJo:
    
    def __init__(self, job_link):
        print("Made it this far!!")
        self.job_link = job_link
    
    #def read_job_data(job_data):
    def convert_csv_data(job_data):
        with open ('job_data.csv', mode='r') as file:
            reader = csv.reader(file)
            csv_data = []
            for row in job_data:
                csv_data.append(row)
                #print(csv_data)
        return 0
    
    def write_to_csv(job_data):
        with open ('job_data.csv', mode='a', newline='') as file:
            writer = csv.writer(file)
            #for row in writer:
            writer.writerow(job_data)
        return 0
    
    
    #@classmethod
    def get_job_info(self, job_link):   #param == cls, job_link
        print("HERE!! GOOD LORD HERE MY CHILD...  FINALLY!")
        job_link = self.job_link
        #print(self.job_link)
        print("Made it to the get_job_info method!")
        return
        job_link = ["https://jobs.lever.co/rover/0a6bcb57-bc8b-4826-b98c-7a17cbb4a911/apply", "https://jobs.lever.co/palantir/e82b696e-a085-4bbf-8bcb-6d2c4f8cf2f7"]
        for job in job_link:
            result = request.get(job)
            content = result.text
            soup = BeautifulSoup(content, 'lxml')
            print(soup.prettify())
            
            if "jobs.lever.co" in job:
                self.lever_io_data(job, soup)
                # Captcha
                # <input id="hcaptchaResponseInput" type="hidden" name="h-captcha-response" value>
                # <button id="hcaptchaSubmitBtn" type="submit" class="hidden"></button>
            
            elif "boards.greenhouse.io" in job:
                self.greenhouse_io(soup)

            elif "workday" in job:
                self.workday(soup)
        self.lever_io_apply(self, "application_link", "application_webpage_html")
        return "ok"
    
    
    #filter out already applied jobs
    #traverse job webpage
    #?????
    def lever_io_data(self, joby_link, soup):
        self.joby_link = joby_link
        
        opening_link_application = soup.find('div', class_='page-application')      #application immediate
        opening_link_description = soup.find('div', class_='page-show')             #regular description start
        
        #if soup.find('a', class_='main-header-logo'):
        if opening_link_application:
            try:
                company_open_positions = soup.find('a', class_='main-header-logo')
                plethora_of_jobs = company_open_positions['href']
                print(plethora_of_jobs)
            except:
                #TODO: Change this Error type!
                raise ConnectionError("ERROR: Companies other open positions are not present")
        elif opening_link_application:
            try:
                position_title = soup.find('h2').get_text()
                job_title = position_title.split()
                print(job_title)
                job_info = job_title.nextSibling('div', class_="posting-categories")
                job_location = job_info.find('div', class_='location')
                job_department = job_info.find('div', class_='department')
                job_commitment = job_info.find('div', class_='commitment')
                job_style = job_info.find('div', class_='workplaceTypes')
        
        
                job_apply_butt = soup.find('a', {'data-qa': 'btn-apply-bottom'})
                link_to_apply = job_apply_butt['href']
                print(link_to_apply)

            except:
                #TODO: Change this Error type!
                raise ConnectionError("ERROR: Companies other open positions are not present")
                
        return soup

    def lever_io_apply(self, application_link, application_webpage_html):
        job_form_html = application_webpage_html.find("form", id="application-form", method="POST")
        application_section_html = job_form_html.find_all("h4")
        #using ^this list .find() 1st <h4> then increment...
        print(application_section_html)
        for user_input in application_section_html:
            #loop through <input> tags and fill in using selenium!!
            print(user_input)
        return 0
    
    def greenhouse_io_data(soup):
        print("Here")
        return 0
    
    def workday_data(soup):
        print("Here")
        return 0
    
    def apply_to_job(job_data: list):
        if (len(job_data)-1):
            return "ok"












------------------------------------------------------------------------------------------------------------------------------------











                                    [       scraperGoogle.py        ]
from bs4 import BeautifulSoup
from selenium import webdriver
#from selenium.webdriver import Firefox, Safari, Chrome
#from dotenv import load_dotenv
from selenium.webdriver.firefox.options import Options as FirefoxOptions
from selenium.webdriver.safari.options import Options as SafariOptions
from selenium.webdriver.chrome.options import Options as ChromeOptions
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By
from selenium.common.exceptions import NoSuchElementException

import time

#I'm running all these inside the Scraper folder so that's why I believe I only need import scraperGoogleJob!!
#import Scraper.scraperGoogleJob
from scraperGoogleJob import scraperGoogleJo
#from scraperGoogleJob import *
#/Users/nliebmann/Desktop/GitHub/Web_Scraper/Scraper/

class scrapeGoogle():
    def __init__(self):
        print(scraperGoogleJo)
        self.browser = None
        self.job_titles = []
        self.list_first_index = 0
        self.list_last_index = 0
    
    def user_requirements(self):
        #global job_titles     #! ERROR: the code didn't like this with .append() for whatever reason!!!?!?!?
        
        print("Well well well. Somebody needs a job... figures someone like you"
              + " is wanting more money! What a weiner your parents raised. You weiner!")
        #time.sleep(1)
        print("Any who just go ahead and type out the job title you want. "
              + " each job title press ENTER! (When you press ENTER it will probably take "
              + "you to the next line, this is normal... illiterately speaking of course)")
        #time.sleep(1)
        print("Try and keep it to around 3-4 you eager eagle. Exceling past these "
              + "numbers will yield less results, so thats why.")
        #time.sleep(1)
        print("Fianlly for the love of the Hoover Dam please spell the everything correctly! "
              + "If you're not confident then highlight the name with your mouse and click\n"
              + " COPY. When you come back here just right click. If you don't have a right"
              + " click well then huh. Life is tough but get your crap together cause idk")
        #time.sleep(1)
        print("When you are done, type ONLY the number of your preferred web browser then press ENTER")
        print(f"\t1) FireFox")
        print(f"\t2) Safari")
        print(f"\t3) Chrome")
        print(f"\t4) Edge")
        while True:
            user_jobs = input()
            user_jobs.strip()
            #if user_jobs == 1:   #! ERROR: comparing a string to an int!!!!
            if user_jobs == "1":
                return 1, " FireFox "
            elif user_jobs == "2":
                return 2, " Safari "
            elif user_jobs == "3":
                return 3, " Chrome "
            elif user_jobs == "4":
                return 4, " Edge "
            else:     #TODO: Make else just check OS and return number of that OS's web browser!!!
                self.job_titles.append(user_jobs)
         
        #TODO       
        # print("Ok now yrs of exp?")
        # user_exp = input()
        # yrs_of_exp.append(user_exp)
        #     -> Add as global like this =>   yrs_of_exp = None  =>  b/c it's an empty string
        # print("Ok and which locations?")
        # location = input()
        # (.env).add(location)    #so it's saved on their system
        # user_location.appemd(location)
        #     ==> When printed in search do   ==>   " & near=" + user_location
    
    def browser_setup(self, test):
        #user_browser_choice, browser_name = self.user_requirements()
        user_browser_choice, browser_name = 2, " Safari "
        self.job_titles.append("software engineer")
        self.job_titles.append("backend engineer")
        print('Execution Started -- Opening' + browser_name + 'Browser')
        
        if user_browser_choice == 1:
            browser = self.browser
            
            options = FirefoxOptions()
            options.set_preference("dom.webnotifications.enabled", False)
            options.set_preference("extensions.enabledScopes", 0)
            options.set_preference("browser.toolbars.bookmarks.visibilty", "never")
            options.set_preference("signon.rememberSignons", False)
            options.set_preference("places.history.enabled", False)
            
            browser = webdriver.Firefox(options=options)
            browser.set_page_load_timeout(30)
        elif user_browser_choice == 2:
            browser = self.browser
            
            options = SafariOptions()
            options.add_argument("--disable-notifications")
            options.add_argument("--disable-extensions")
            options.add_argument("--disable-infobars")
                
            browser = webdriver.Safari(options=options)
            browser.set_page_load_timeout(30)
        elif user_browser_choice == 3:
            browser = self.browser
            
            options = ChromeOptions()
            options.add_argument("--disable-notifications")
            options.add_argument("--disable-extensions")
            options.add_argument("--disable-infobars")
            
            browser = webdriver.Chrome(options=options)
            browser.set_page_load_timeout(30)
        
        if(test == 0):
            browser.get('https://www.google.com')
            self.search_for_jobs(browser)
        else:
            raise ConnectionError('ERROR: Check Internet Connection')
        
        browser.quit()
        print('Execution Ending -- Webdriver session is Closing')
        
    def search_for_jobs(self, browser):
        job_titles = self.job_titles
        
        print('Searching for ' + ", ".join(job_titles) + ' jobs...    you lazy son of 21 guns')
        search_bar = browser.find_element(By.NAME, "q")
        search_bar.clear()
        search_bar.send_keys('site:lever.co | site:greenhouse.io | site:workday.com')
        print('1/2')
        time.sleep(1)
        job_titles_string = ' ("'
        for i, job in enumerate(job_titles):
            if i == len(job_titles)-1 or len(job_titles) == 0:
                job_titles_string += (job + '")')
            else:
                job_titles_string += (job + '" | "')
        search_bar.send_keys(job_titles_string)
        print('2/2')
        print("Searching google for...       adult films?")
        time.sleep(1)
        #TODO: Uncomment below and erease    .search_time_frame() !!!!
        #self.search_locations(self, browser, search_bar)
        self.search_time_frame(browser, search_bar)
        return
    
    #TODO
    def search_locations(self, browser, search_bar):
        global good_locations
        global bad_locations
        
        #NOTE: [if not variable] checks if the length of variable is = to 0; variable here is a 'list[]' too!! 
        if not good_locations and not bad_locations:
            self.search_time_frame(self, browser)
        
        #NOTE: HERE add SPACE to the BEGININNG because we don't care about the end!!!
        search_location = " & "
        for count, add_location in enumerate(good_locations):
            if count == len(good_locations):
                search_location += (" near=" + add_location + " ")
                #! ADD: Find out how to add more location!!!!!                
        for count, exclude_location in bad_locations:
            if count == len(bad_locations):
                search_location += ("!(near=" + exclude_location + ")")
        
        search_bar.send_keys(search_location)
        self.search_time_frame(self, browser, search_bar)
        return
    
    def search_time_frame(self, browser, search_bar):
        search_bar.send_keys(Keys.RETURN)
        print("TAAAADDDAAAAAA")
        time.sleep(1)
        
        tools_butt = browser.find_element(By.XPATH, "//div[text()='Tools']")
        tools_butt.click()
        
        any_time_butt = browser.find_element(By.XPATH, "//div[text()='Any time']")
        any_time_butt.click()
        decisi = "24"
        
        if decisi == "24":
            past_24 = browser.find_element(By.XPATH, "//a[text()='Past 24 hours']")
            past_24.click()
        elif decisi == "7":
            past_week = browser.find_element(By.XPATH, "//a[text()='Past week']")
            past_week.click()
        else:
            raise TypeError('ERROR: Didnt pick a registered time!')
        print("Filtering by past " + decisi)
        time.sleep(1)
        self.search_results(browser, self.list_first_index, self.list_last_index)
        return
        
    def search_results(self, browser, list_first_index, list_last_index):
        if list_first_index == 0:
            search_results = browser.find_elements(By.CSS_SELECTOR, f"div.g:nth-child(n+{list_first_index})")
            print(f"Number of search results: {len(search_results)}")
            list_last_index = len(search_results)
            
        if list_first_index == 0:
            search_results = browser.find_elements(By.CSS_SELECTOR, f"div.g:nth-child(n+{list_first_index})")
            print(f"Number of search results: {len(search_results)}")
            list_last_index = len(search_results)
        else:
            search_results = browser.find_elements(By.CSS_SELECTOR, f"div.g:nth-child(n+{list_first_index+1})")
        
        for count, results_link in enumerate(search_results, list_first_index):
            print('--------------------------------')
            print(str(count+1) + "/" + str(list_last_index))
            print(results_link)
            link = results_link.find_element(By.CSS_SELECTOR, "a")  #"h3.LC201b > a"
            print(f"Here is link #{count+1}: ", end="")
            job_link = link.get_attribute("href")
            #print(link.get_attribute("href"))
            print(job_link)
            #$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
            # scraperGoogleJob(job_link)
            #$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$$
            
            if count == list_last_index:
                list_first_index = list_last_index
                break
        print("All done loser!")
        time.sleep(1)
        self.increment_search_results(browser, list_first_index, list_last_index)
        return list_first_index, list_last_index
    
    def increment_search_results(self, browser, list_first_index, list_last_index):
        current_height = browser.execute_script("return document.body.scrollHeight")
        print('\n\n\n')
        print("increment_search_results")
        print("****************************************************************")
        print("Current Height == " + str(current_height))
        
        while True:
            browser.execute_script("window.scrollTo(0, document.body.scrollHeight);")
            time.sleep(1)
            print("Scrolled...")
            
            #search_results = browser.find_elements(By.CSS_SELECTOR, "div.g")
            current_list_length = list_last_index-list_first_index
            #**************************************************************************************************************
            if (current_list_length%100) == 1:
                print("Length of the current list == " + str(current_list_length))
            try:
                no_more_results = browser.find_element(By.XPATH, "//a[text()='repeat the search with the omitted results included']")
                print("No more search results")
                break
            except NoSuchElementException:
                pass
            #**************************************************************************************************************
            new_height = browser.execute_script("return document.body.scrollHeight")
            print("New Height == " + str(new_height))
            
            if new_height == current_height:
                try:
                    more_results = browser.find_element(By.XPATH, "//span[text()='More results']")
                    if more_results:
                        print("Found the more_results == ", end="")
                        print(more_results)
                        more_results.click()
                        print("Clicked 'More results' button")
                        time.sleep(1)
                    elif not more_results:
                        print("NOTHING == more_results")
                except NoSuchElementException:
                    return  ("ERROR: Didn't work I guess idk??")
                
            current_height = new_height
            list_first_index, list_last_index = self.search_results(browser, list_first_index, list_last_index)
            print("Current height == " + str(current_height))
        print("****************************************************************")
        print('\n\n\n')
        
        print("Scrolled to the end of search results, GOOBER!")
        time.sleep(5)
        print("++++++++++++++++++++++++++++++++++++++++++++++")
        job_link = "https://www.google.com"
        scraperGoogleJo.get_job_info(self, job_link)
        #job_link = scraperGoogleJob.get_job_info(job_link)
        #scraperGoogleJob(job_link)
        print("++++++++++++++++++++++++++++++++++++++++++++++")
        return
        

if __name__ == '__main__':
    scraper = scrapeGoogle()
    scraper.browser_setup(0)







#site:lever.co | site:greenhouse.io | site:workday.com ("Software Engineer" | "Backend Engineer") -Senior -Sr location:us






# In order to show you the most relevant results, we have omitted some entries very similar to
# the 225 already displayed.
# If you like, you can "repeat the search with the omitted results included."







# HOW TO SET UP SAFARI !!!!!
#    1) Open Safari.
#    2) Click on Safari in the top menu bar.
#    3) Click on Preferences.
#    4) Click on Advanced.
#    5) At the bottom, check the box next to "Show Develop menu in menu bar".
#    6) Click on Develop in the top menu bar.
#    7) Click on Allow Remote Automation.












------------------------------------------------------------------------------------------------------------------------------------











from urllib import request
from urllib.parse import urlparse, parse_qs
#from bs4 import BeautifulSoup
import csv

from selenium import webdriver
from selenium.webdriver.firefox.options import Options as FirefoxOptions
from selenium.webdriver.safari.options import Options as SafariOptions
from selenium.webdriver.chrome.options import Options as ChromeOptions
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By

from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.action_chains import ActionChains
import time

class scraperGoogleJob:
    
    # def __init__(self, job_link):
    #     print("Made it this far!!")
    #     self.job_link = job_link
    
    #def read_job_data(job_data):
    def convert_csv_data(job_data):
        with open ('job_data.csv', mode='r') as file:
            reader = csv.reader(file)
            csv_data = []
            for row in job_data:
                csv_data.append(row)
                #print(csv_data)
        return 0
    
    def write_to_csv(job_data):
        with open ('job_data.csv', mode='a', newline='') as file:
            writer = csv.writer(file)
            #for row in writer:
            writer.writerow(job_data)
        return 0
    
    
    #@classmethod
    def get_job_info(var_job_link, browser):   #param == cls, job_link  |  self, var_search_results
        search_results = var_job_link
        #job_link = ["https://jobs.lever.co/rover/0a6bcb57-bc8b-4826-b98c-7a17cbb4a911/apply", "https://jobs.lever.co/palantir/e82b696e-a085-4bbf-8bcb-6d2c4f8cf2f7"]
        print(search_results)
        print("Length of search_results is ", end="")
        print(len(search_results))
        #for job in range(len(search_results)-1, -1, -1):
        for job in search_results[::-1]:
            print(job)
            #NOTE: f"//a[@href='{job}']" NEEDS the @
            #scroll_to_link = WebDriverWait(browser, 10).until(EC.presence_of_element_located((By.XPATH, f"//a[@href='{job}']")))
            scroll_to_link = WebDriverWait(browser, 10).until(EC.presence_of_element_located((By.XPATH, f"//a[@href='{job}']")))
            print(scroll_to_link)
            # scroll_to_link = browser.find_element(By.XPATH, f"//a[href='{job}']")
            browser.execute_script("arguments[0].scrollIntoView();", scroll_to_link)
            
            
            #decode URL if necessary
            if job.startswith('/url?'):
                url_start_index = job.find('http')
                url_end_index = job.find('&', url_start_index)
                job_url = job[url_start_index:url_end_index]
            else:
                job_url = job
            jb_link = browser.find_element(By.XPATH, f"//a[@href='{job_url}']")
            job_button = browser.find_element(jb_link, "h3")
            job_button.click()
            
            #time.sleep(3)
            #scroll to URL on webpage and click it
            #scroll_to_link = browser.find_element(By.XPATH, f"//a[href='{job_url}']")
            #browser.execute_script("arguments[0].scrollIntoView();", scroll_to_link)
            #ActionChains(browser).move_to_element(scroll_to_link).click().perform()
            #scroll_to_link.click()
            #browser.click(job_url)
            browser.implicitly_wait(5)
            print("Waiting for link to load...")
            
            #get HTML from clicked webpage
            result = request.get(job)
            content = result.text
            soup = BeautifulSoup(content, 'lxml')
            print(soup.prettify())
            
            if "jobs.lever.co" in job:
                self.lever_io_data(job, soup)
                #scraperGoogleJob.lever_io_data(job, soup)
                # Captcha
                # <input id="hcaptchaResponseInput" type="hidden" name="h-captcha-response" value>
                # <button id="hcaptchaSubmitBtn" type="submit" class="hidden"></button>
            
            elif "boards.greenhouse.io" in job:
                self.greenhouse_io(soup)

            elif "workday" in job:
                self.workday(soup)
        print("Well sue me silly!")
        self.lever_io_apply(self, "application_link", "application_webpage_html")
        return "ok"
    
    
    #filter out already applied jobs
    #traverse job webpage
    #?????
    def lever_io_data(self, joby_link, soup):
        self.joby_link = joby_link
        
        opening_link_application = soup.find('div', class_='page-application')      #application immediate
        opening_link_description = soup.find('div', class_='page-show')             #regular description start
        
        #if soup.find('a', class_='main-header-logo'):
        if opening_link_application:
            try:
                company_open_positions = soup.find('a', class_='main-header-logo')
                plethora_of_jobs = company_open_positions['href']
                print(plethora_of_jobs)
            except:
                #TODO: Change this Error type!
                raise ConnectionError("ERROR: Companies other open positions are not present")
        elif opening_link_application:
            try:
                position_title = soup.find('h2').get_text()
                job_title = position_title.split()
                print(job_title)
                job_info = job_title.nextSibling('div', class_="posting-categories")
                job_location = job_info.find('div', class_='location')
                job_department = job_info.find('div', class_='department')
                job_commitment = job_info.find('div', class_='commitment')
                job_style = job_info.find('div', class_='workplaceTypes')
        
        
                job_apply_butt = soup.find('a', {'data-qa': 'btn-apply-bottom'})
                link_to_apply = job_apply_butt['href']
                print(link_to_apply)

            except:
                #TODO: Change this Error type!
                raise ConnectionError("ERROR: Companies other open positions are not present")
                
        return soup

    def lever_io_apply(self, application_link, application_webpage_html):
        job_form_html = application_webpage_html.find("form", id="application-form", method="POST")
        application_section_html = job_form_html.find_all("h4")
        #using ^this list .find() 1st <h4> then increment...
        print(application_section_html)
        for user_input in application_section_html:
            #loop through <input> tags and fill in using selenium!!
            print(user_input)
        return 0
    
    def greenhouse_io_data(soup):
        print("Here")
        return 0
    
    def workday_data(soup):
        print("Here")
        return 0
    
    def apply_to_job(job_data: list):
        if (len(job_data)-1):
            return "ok"









#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
#!       REMEMBER TO COUNT THE NUMBER OF OPEN SENIOR > ROLES AVAILABLE           !
#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!



#<cite class="qLRx3b tjvcx GvPZzd cHaqb" role="text" style="max-width:315px">https://jobs.lever.co<span class="dyjrff qzEoUe" role="text"> › ltaresearch</span></cite>

#<a href="/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;ved=2ahUKEwiJt8e-kpj-AhXdnWoFHTlfAxE4WhAWegQIBxAB&amp;url=https%3A%2F%2Fjobs.lever.co%2Fltaresearch&amp;usg=AOvVaw1WnH3yWFh2qyF8H83db1P7" data-jsarwt="1" data-usg="AOvVaw1WnH3yWFh2qyF8H83db1P7" data-ved="2ahUKEwiJt8e-kpj-AhXdnWoFHTlfAxE4WhAWegQIBxAB" data-ctbtn="0" data-cthref="/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;ved=2ahUKEwiJt8e-kpj-AhXdnWoFHTlfAxE4WhAWegQIBxAB&amp;url=https%3A%2F%2Fjobs.lever.co%2Fltaresearch&amp;usg=AOvVaw1WnH3yWFh2qyF8H83db1P7" data-jrwt="1"><br><h3 class="LC20lb MBeuO DKV0Md">LTA Research</h3><div class="TbwUpd NJjxre iUh30 ojE3Fb"><span class="H9lube"><div class="eqA2re NjwKYd Vwoesf" aria-hidden="true"><img class="XNo5Ab" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAMAAABEpIrGAAAAWlBMVEU+SEpvdXjIysvEx8edoaN4f4FOV1n6+vr////j5eVjamxyeXvZ2dn39/dqcXPNz9CLkJCqra7y8/PU1NRdZWfs7e7d3t5cY2ZTW12zt7hYXmKTmJp/hYe6vLweb9tYAAAAn0lEQVR4AcTPAxLAMBBA0TKszfsfs1acYf9w54XOT7meH4RA7SFEe5gonJ6OfF/vXhQBqcePR4nE3cvTaCsTPfl6lBtcXJAUp5enl8TgCefE0iPF+SavajtvFN6ynpq84rwzuNOfjlXuRIy3jnwB46FP+AWMZxAVCbvgcm/Y5xFtxfS7gPEAHcHpXVB/PUd32fPN+Sjo9oFGb906+uRTAOgAEo+qriNyAAAAAElFTkSuQmCC" style="height:18px;width:18px" alt=""></div></span><div><span class="VuuXrf">lever.co</span><div class="byrV5b"><cite class="qLRx3b tjvcx GvPZzd cHaqb" role="text" style="max-width:315px">https://jobs.lever.co<span class="dyjrff qzEoUe" role="text"> › ltaresearch</span></cite></div></div></div></a>




#<a href="/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;ved=2ahUKEwiJt8e-kpj-AhXdnWoFHTlfAxE4WhAWegQIBxAB&amp;url=https%3A%2F%2Fjobs.lever.co%2Fltaresearch&amp;usg=AOvVaw1WnH3yWFh2qyF8H83db1P7" data-jsarwt="1" data-usg="AOvVaw1WnH3yWFh2qyF8H83db1P7" data-ved="2ahUKEwiJt8e-kpj-AhXdnWoFHTlfAxE4WhAWegQIBxAB" data-ctbtn="0" data-cthref="/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;ved=2ahUKEwiJt8e-kpj-AhXdnWoFHTlfAxE4WhAWegQIBxAB&amp;url=https%3A%2F%2Fjobs.lever.co%2Fltaresearch&amp;usg=AOvVaw1WnH3yWFh2qyF8H83db1P7" data-jrwt="1"><br><h3 class="LC20lb MBeuO DKV0Md">LTA Research</h3>












------------------------------------------------------------------------------------------------------------------------------------











from urllib import request
from urllib.parse import urlparse, parse_qs
#from bs4 import BeautifulSoup
import csv

from selenium import webdriver
from selenium.webdriver.firefox.options import Options as FirefoxOptions
from selenium.webdriver.safari.options import Options as SafariOptions
from selenium.webdriver.chrome.options import Options as ChromeOptions
from selenium.webdriver.common.keys import Keys
from selenium.webdriver.common.by import By

from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.common.action_chains import ActionChains
import time

class scraperGoogleJob:
    
    # def __init__(self, job_link):
    #     print("Made it this far!!")
    #     self.job_link = job_link
    
    #def read_job_data(job_data):
    def convert_csv_data(job_data):
        with open ('job_data.csv', mode='r') as file:
            reader = csv.reader(file)
            csv_data = []
            for row in job_data:
                csv_data.append(row)
                #print(csv_data)
        return 0
    
    def write_to_csv(job_data):
        with open ('job_data.csv', mode='a', newline='') as file:
            writer = csv.writer(file)
            #for row in writer:
            writer.writerow(job_data)
        return 0
    
    
    #@classmethod
    def get_job_info(var_job_link, browser, google_search_button):   #param == cls, job_link  |  self, var_search_results
        search_results = var_job_link
        #job_link = ["https://jobs.lever.co/rover/0a6bcb57-bc8b-4826-b98c-7a17cbb4a911/apply", "https://jobs.lever.co/palantir/e82b696e-a085-4bbf-8bcb-6d2c4f8cf2f7"]
        print(search_results)
        print("Length of search_results is ", end="")
        print(len(search_results))
        #for job in range(len(search_results)-1, -1, -1):
        for job in search_results[::-1]:
            # if job.startswith('/url?'):
            #     url_start_index = job.find('http')
            #     url_end_index = job.find('&', url_start_index)
            #     job_url = job[url_start_index:url_end_index]
            # else:
            #     job_url = job
            # a_tag = browser.find_element(By.XPATH, "//a")
            print(google_search_button)
            #click_job_link = job.find_element(By.CSS_SELECTOR, google_search_button)
            #parent = job.find_element(By.XPATH, "./ancestor::h3")
            #linky3 = parent.find_element(By.XPATH, ".//a")
            #linky3.click()
            link_elemen = browser.find_element(By.XPATH, f'//ancestor::a/h3[text()="{google_search_button}"]')
            print(link_elemen)
            link_elemen.click()
            print("Waiting for link to load...")
            browser.implicitly_wait(5)
            time.sleep(15)
            
            #get HTML from clicked webpage
            result = request.get(job)
            content = result.text
            soup = BeautifulSoup(content, 'lxml')
            print(soup.prettify())
            
            if "jobs.lever.co" in job:
                self.lever_io_data(job, soup)
                #scraperGoogleJob.lever_io_data(job, soup)
                # Captcha
                # <input id="hcaptchaResponseInput" type="hidden" name="h-captcha-response" value>
                # <button id="hcaptchaSubmitBtn" type="submit" class="hidden"></button>
            
            elif "boards.greenhouse.io" in job:
                self.greenhouse_io(soup)

            elif "workday" in job:
                self.workday(soup)
        print("Well sue me silly!")
        self.lever_io_apply(self, "application_link", "application_webpage_html")
        return "ok"
    
    
    #filter out already applied jobs
    #traverse job webpage
    #?????
    def lever_io_data(self, joby_link, soup):
        self.joby_link = joby_link
        
        opening_link_application = soup.find('div', class_='page-application')      #application immediate
        opening_link_description = soup.find('div', class_='page-show')             #regular description start
        
        #if soup.find('a', class_='main-header-logo'):
        if opening_link_application:
            try:
                company_open_positions = soup.find('a', class_='main-header-logo')
                plethora_of_jobs = company_open_positions['href']
                print(plethora_of_jobs)
            except:
                #TODO: Change this Error type!
                raise ConnectionError("ERROR: Companies other open positions are not present")
        elif opening_link_application:
            try:
                position_title = soup.find('h2').get_text()
                job_title = position_title.split()
                print(job_title)
                job_info = job_title.nextSibling('div', class_="posting-categories")
                job_location = job_info.find('div', class_='location')
                job_department = job_info.find('div', class_='department')
                job_commitment = job_info.find('div', class_='commitment')
                job_style = job_info.find('div', class_='workplaceTypes')
        
        
                job_apply_butt = soup.find('a', {'data-qa': 'btn-apply-bottom'})
                link_to_apply = job_apply_butt['href']
                print(link_to_apply)

            except:
                #TODO: Change this Error type!
                raise ConnectionError("ERROR: Companies other open positions are not present")
                
        return soup

    def lever_io_apply(self, application_link, application_webpage_html):
        job_form_html = application_webpage_html.find("form", id="application-form", method="POST")
        application_section_html = job_form_html.find_all("h4")
        #using ^this list .find() 1st <h4> then increment...
        print(application_section_html)
        for user_input in application_section_html:
            #loop through <input> tags and fill in using selenium!!
            print(user_input)
        return 0
    
    def greenhouse_io_data(soup):
        print("Here")
        return 0
    
    def workday_data(soup):
        print("Here")
        return 0
    
    def apply_to_job(job_data: list):
        if (len(job_data)-1):
            return "ok"









#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!
#!       REMEMBER TO COUNT THE NUMBER OF OPEN SENIOR > ROLES AVAILABLE           !
#!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!!



#<cite class="qLRx3b tjvcx GvPZzd cHaqb" role="text" style="max-width:315px">https://jobs.lever.co<span class="dyjrff qzEoUe" role="text"> › ltaresearch</span></cite>

#<a href="/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;ved=2ahUKEwiJt8e-kpj-AhXdnWoFHTlfAxE4WhAWegQIBxAB&amp;url=https%3A%2F%2Fjobs.lever.co%2Fltaresearch&amp;usg=AOvVaw1WnH3yWFh2qyF8H83db1P7" data-jsarwt="1" data-usg="AOvVaw1WnH3yWFh2qyF8H83db1P7" data-ved="2ahUKEwiJt8e-kpj-AhXdnWoFHTlfAxE4WhAWegQIBxAB" data-ctbtn="0" data-cthref="/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;ved=2ahUKEwiJt8e-kpj-AhXdnWoFHTlfAxE4WhAWegQIBxAB&amp;url=https%3A%2F%2Fjobs.lever.co%2Fltaresearch&amp;usg=AOvVaw1WnH3yWFh2qyF8H83db1P7" data-jrwt="1"><br><h3 class="LC20lb MBeuO DKV0Md">LTA Research</h3><div class="TbwUpd NJjxre iUh30 ojE3Fb"><span class="H9lube"><div class="eqA2re NjwKYd Vwoesf" aria-hidden="true"><img class="XNo5Ab" src="data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAACAAAAAgCAMAAABEpIrGAAAAWlBMVEU+SEpvdXjIysvEx8edoaN4f4FOV1n6+vr////j5eVjamxyeXvZ2dn39/dqcXPNz9CLkJCqra7y8/PU1NRdZWfs7e7d3t5cY2ZTW12zt7hYXmKTmJp/hYe6vLweb9tYAAAAn0lEQVR4AcTPAxLAMBBA0TKszfsfs1acYf9w54XOT7meH4RA7SFEe5gonJ6OfF/vXhQBqcePR4nE3cvTaCsTPfl6lBtcXJAUp5enl8TgCefE0iPF+SavajtvFN6ynpq84rwzuNOfjlXuRIy3jnwB46FP+AWMZxAVCbvgcm/Y5xFtxfS7gPEAHcHpXVB/PUd32fPN+Sjo9oFGb906+uRTAOgAEo+qriNyAAAAAElFTkSuQmCC" style="height:18px;width:18px" alt=""></div></span><div><span class="VuuXrf">lever.co</span><div class="byrV5b"><cite class="qLRx3b tjvcx GvPZzd cHaqb" role="text" style="max-width:315px">https://jobs.lever.co<span class="dyjrff qzEoUe" role="text"> › ltaresearch</span></cite></div></div></div></a>




#<a href="/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;ved=2ahUKEwiJt8e-kpj-AhXdnWoFHTlfAxE4WhAWegQIBxAB&amp;url=https%3A%2F%2Fjobs.lever.co%2Fltaresearch&amp;usg=AOvVaw1WnH3yWFh2qyF8H83db1P7" data-jsarwt="1" data-usg="AOvVaw1WnH3yWFh2qyF8H83db1P7" data-ved="2ahUKEwiJt8e-kpj-AhXdnWoFHTlfAxE4WhAWegQIBxAB" data-ctbtn="0" data-cthref="/url?sa=t&amp;rct=j&amp;q=&amp;esrc=s&amp;source=web&amp;cd=&amp;ved=2ahUKEwiJt8e-kpj-AhXdnWoFHTlfAxE4WhAWegQIBxAB&amp;url=https%3A%2F%2Fjobs.lever.co%2Fltaresearch&amp;usg=AOvVaw1WnH3yWFh2qyF8H83db1P7" data-jrwt="1"><br><h3 class="LC20lb MBeuO DKV0Md">LTA Research</h3>












------------------------------------------------------------------------------------------------------------------------------------











werwerwer












------------------------------------------------------------------------------------------------------------------------------------











werwerwer












------------------------------------------------------------------------------------------------------------------------------------











werwerwer












------------------------------------------------------------------------------------------------------------------------------------











werwerwer












------------------------------------------------------------------------------------------------------------------------------------











werwerwer












------------------------------------------------------------------------------------------------------------------------------------











werwerwer












------------------------------------------------------------------------------------------------------------------------------------











werwerwer












------------------------------------------------------------------------------------------------------------------------------------











werwerwer












------------------------------------------------------------------------------------------------------------------------------------































